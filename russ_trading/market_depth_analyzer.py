#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸‚åœºæ·±åº¦åˆ†æå™¨
æä¾›æ¿å—è½®åŠ¨ã€å¸‚åœºæƒ…ç»ªã€é‡ä»·å…³ç³»ç­‰æ·±åº¦ç›˜é¢åˆ†æ

åŠŸèƒ½:
1. æ¿å—è½®åŠ¨åˆ†æ (é¢†æ¶¨/é¢†è·Œæ¿å—, èµ„é‡‘æµå‘)
2. å¸‚åœºæƒ…ç»ªæ¸©åº¦è®¡ (æ¶¨è·Œåœ, æ¶¨è·Œå®¶æ•°, åŒ—å‘èµ„é‡‘)
3. é‡ä»·å…³ç³»æ·±åº¦åˆ†æ (é‡æ¯”, ä¸»åŠ›èµ„é‡‘)

Author: Claude Code
Created: 2025-10-24
Updated: 2025-10-25 (æ·»åŠ é‡è¯•å’Œç¼“å­˜æœºåˆ¶)
"""

import logging
import time
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from functools import wraps
import akshare as ak
import efinance as ef

logger = logging.getLogger(__name__)


def retry_on_error(max_retries=3, delay=2):
    """
    é‡è¯•è£…é¥°å™¨ - ç”¨äºå¤„ç†ç½‘ç»œè¯·æ±‚å¤±è´¥

    Args:
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"{func.__name__} å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                        time.sleep(delay)
                    else:
                        logger.error(f"{func.__name__} æœ€ç»ˆå¤±è´¥: {e}")
                        raise
            return None
        return wrapper
    return decorator


class MarketDepthAnalyzer:
    """å¸‚åœºæ·±åº¦åˆ†æå™¨ï¼ˆæ”¯æŒé‡è¯•å’Œç¼“å­˜ï¼‰"""

    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º data/cache
        """
        self.logger = logger

        # è®¾ç½®ç¼“å­˜ç›®å½•
        if cache_dir is None:
            project_root = Path(__file__).parent.parent
            cache_dir = project_root / 'data' / 'cache'

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"ç¼“å­˜ç›®å½•: {self.cache_dir}")

    def _get_cache_path(self, cache_name: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_name}.json"

    def _save_cache(self, cache_name: str, data: Dict) -> None:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(cache_name)
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'data': data
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            self.logger.debug(f"ç¼“å­˜å·²ä¿å­˜: {cache_name}")
        except Exception as e:
            self.logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _load_cache(self, cache_name: str, max_age_hours: int = 24) -> Optional[Dict]:
        """
        ä»ç¼“å­˜åŠ è½½æ•°æ®

        Args:
            cache_name: ç¼“å­˜åç§°
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é•¿ï¼ˆå°æ—¶ï¼‰

        Returns:
            ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æœç¼“å­˜è¿‡æœŸæˆ–ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        try:
            cache_path = self._get_cache_path(cache_name)
            if not cache_path.exists():
                return None

            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['timestamp'])
            age = datetime.now() - cache_time

            if age > timedelta(hours=max_age_hours):
                self.logger.debug(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_name} (å¹´é¾„: {age})")
                return None

            self.logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {cache_name} (ç¼“å­˜æ—¶é—´: {cache_time.strftime('%Y-%m-%d %H:%M:%S')})")
            return cache_data['data']

        except Exception as e:
            self.logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None

    def analyze_sector_rotation(self, date: str = None) -> Dict:
        """
        åˆ†ææ¿å—è½®åŠ¨ï¼ˆæ”¯æŒç¼“å­˜å’Œé‡è¯•ï¼‰

        Args:
            date: æ—¥æœŸ (YYYY-MM-DD), é»˜è®¤ä»Šå¤©

        Returns:
            æ¿å—è½®åŠ¨åˆ†æç»“æœ
        """
        cache_name = 'sector_rotation'
        result = {
            'top_gainers': [],      # é¢†æ¶¨æ¿å—
            'top_losers': [],       # é¢†è·Œæ¿å—
            'capital_flow': [],     # èµ„é‡‘æµå‘
            'rotation_signal': '',  # è½®åŠ¨ä¿¡å·
            'analysis': '',         # ç»¼åˆåˆ†æ
            'data_source': 'realtime'  # æ•°æ®æ¥æºæ ‡è¯†
        }

        # å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
        data_fetched = False

        try:
            # 1. è·å–æ¿å—è¡Œæƒ…æ•°æ®
            self.logger.info("è·å–æ¿å—è¡Œæƒ…æ•°æ®...")

            # ä½¿ç”¨ä¸œæ–¹è´¢å¯Œçš„è¡Œä¸šæ¿å—æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
            df_industry = self._fetch_concept_board_with_retry()

            if df_industry is not None and len(df_industry) > 0:
                # æŒ‰æ¶¨è·Œå¹…æ’åº
                df_industry = df_industry.sort_values('æ¶¨è·Œå¹…', ascending=False)

                # é¢†æ¶¨æ¿å— (å‰5)
                for idx, row in df_industry.head(5).iterrows():
                    result['top_gainers'].append({
                        'name': row['æ¿å—åç§°'],
                        'change_pct': float(row['æ¶¨è·Œå¹…']),
                        'leader': row.get('é¢†æ¶¨è‚¡ç¥¨', 'N/A')
                    })

                # é¢†è·Œæ¿å— (å5)
                for idx, row in df_industry.tail(5).iterrows():
                    result['top_losers'].append({
                        'name': row['æ¿å—åç§°'],
                        'change_pct': float(row['æ¶¨è·Œå¹…']),
                        'leader': row.get('é¢†æ¶¨è‚¡ç¥¨', 'N/A')
                    })

                self.logger.info(f"âœ… è·å–åˆ° {len(df_industry)} ä¸ªè¡Œä¸šæ¿å—æ•°æ®")
                data_fetched = True

        except Exception as e:
            self.logger.warning(f"è·å–è¡Œä¸šæ¿å—æ•°æ®å¤±è´¥: {e}")

        # 2. è·å–æ¿å—èµ„é‡‘æµå‘
        if data_fetched:  # åªæœ‰æ¿å—æ•°æ®è·å–æˆåŠŸæ—¶æ‰å°è¯•èµ„é‡‘æµå‘
            try:
                df_flow = self._fetch_capital_flow_with_retry()
                if df_flow is not None and len(df_flow) > 0:
                    for idx, row in df_flow.head(5).iterrows():
                        result['capital_flow'].append({
                            'sector': row['åç§°'],
                            'net_inflow': float(row['ä¸»åŠ›å‡€æµå…¥-å‡€é¢']) / 1e8,  # è½¬æ¢ä¸ºäº¿
                            'net_inflow_pct': float(row['ä¸»åŠ›å‡€æµå…¥-å‡€å æ¯”'])
                        })
                    self.logger.info(f"âœ… è·å–èµ„é‡‘æµå‘æ•°æ®")
            except Exception as e:
                self.logger.warning(f"è·å–èµ„é‡‘æµå‘å¤±è´¥: {e}")

        # 3. å¦‚æœå®æ—¶æ•°æ®è·å–æˆåŠŸï¼Œä¿å­˜åˆ°ç¼“å­˜
        if data_fetched:
            self._save_cache(cache_name, result)
        else:
            # 4. å®æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç¼“å­˜
            self.logger.warning("âš ï¸ å®æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç¼“å­˜æ•°æ®...")
            cached_data = self._load_cache(cache_name, max_age_hours=72)  # å…è®¸ä½¿ç”¨3å¤©å†…çš„ç¼“å­˜

            if cached_data:
                result = cached_data
                result['data_source'] = 'cache'
                self.logger.info("âœ… å·²ä½¿ç”¨ç¼“å­˜æ•°æ®")
            else:
                self.logger.error("âŒ æ— å¯ç”¨ç¼“å­˜ï¼Œè¿”å›ç©ºæ•°æ®")
                result['rotation_signal'] = "æ•°æ®è·å–å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰"
                result['analysis'] = "æ— æ³•è¿æ¥æ•°æ®æºï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
                return result

        # 5. åˆ¤æ–­è½®åŠ¨ä¿¡å·
        result['rotation_signal'] = self._analyze_rotation_signal(result)

        # 6. ç”Ÿæˆç»¼åˆåˆ†æ
        result['analysis'] = self._generate_rotation_analysis(result)

        return result

    @retry_on_error(max_retries=3, delay=2)
    def _fetch_concept_board_with_retry(self):
        """å¸¦é‡è¯•çš„è¡Œä¸šæ¿å—æ•°æ®è·å–"""
        self.logger.info("æ­£åœ¨è·å–è¡Œä¸šæ¿å—æ•°æ®...")
        return ak.stock_board_industry_name_em()

    @retry_on_error(max_retries=3, delay=2)
    def _fetch_capital_flow_with_retry(self):
        """å¸¦é‡è¯•çš„èµ„é‡‘æµå‘æ•°æ®è·å–"""
        self.logger.info("æ­£åœ¨è·å–èµ„é‡‘æµå‘æ•°æ®...")
        return ak.stock_sector_fund_flow_rank(indicator="ä»Šæ—¥")

    @retry_on_error(max_retries=3, delay=2)
    def _fetch_realtime_quotes_with_retry(self):
        """å¸¦é‡è¯•çš„å®æ—¶è¡Œæƒ…æ•°æ®è·å–"""
        self.logger.info("æ­£åœ¨è·å–å®æ—¶è¡Œæƒ…æ•°æ®...")
        return ak.stock_zh_a_spot_em()

    def _analyze_rotation_signal(self, data: Dict) -> str:
        """åˆ†æè½®åŠ¨ä¿¡å·"""
        try:
            if not data['top_gainers']:
                return "æ•°æ®ä¸è¶³"

            # æ£€æŸ¥é¢†æ¶¨æ¿å—ç±»å‹ï¼ˆé€‚é…è¡Œä¸šæ¿å—å‘½åï¼‰
            tech_sectors = ['åŠå¯¼ä½“', 'ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡', 'ä¼ åª’', 'æ–°èƒ½æº', 'ç”µåŠ›è®¾å¤‡']
            defensive_sectors = ['é“¶è¡Œ', 'ä¿é™©', 'é£Ÿå“é¥®æ–™', 'å…¬ç”¨äº‹ä¸š', 'æˆ¿åœ°äº§']
            cycle_sectors = ['æœ‰è‰²é‡‘å±', 'é’¢é“', 'ç…¤ç‚­', 'åŒ–å·¥', 'å»ºæ']

            top_gainer = data['top_gainers'][0]['name'] if data['top_gainers'] else ''

            # åˆ¤æ–­è¿›æ”»/é˜²å®ˆ/å‘¨æœŸåˆ‡æ¢
            is_tech_leading = any(s in top_gainer for s in tech_sectors)
            is_defensive_leading = any(s in top_gainer for s in defensive_sectors)
            is_cycle_leading = any(s in top_gainer for s in cycle_sectors)

            if is_tech_leading:
                return "âœ… è¿›æ”»ä¿¡å· - ç§‘æŠ€/æˆé•¿æ¿å—é¢†æ¶¨"
            elif is_defensive_leading:
                return "âš ï¸ é˜²å¾¡ä¿¡å· - é˜²å¾¡æ€§æ¿å—é¢†æ¶¨"
            elif is_cycle_leading:
                return "ğŸ”„ å‘¨æœŸä¿¡å· - å‘¨æœŸæ€§æ¿å—é¢†æ¶¨"
            else:
                return "â¡ï¸ ä¸­æ€§ä¿¡å· - æ¿å—è½®åŠ¨ä¸æ˜æ˜¾"

        except Exception as e:
            self.logger.warning(f"è½®åŠ¨ä¿¡å·åˆ†æå¤±è´¥: {e}")
            return "åˆ†æå¤±è´¥"

    def _generate_rotation_analysis(self, data: Dict) -> str:
        """ç”Ÿæˆæ¿å—è½®åŠ¨ç»¼åˆåˆ†æ"""
        try:
            lines = []

            # é¢†æ¶¨åˆ†æ
            if data['top_gainers']:
                top = data['top_gainers'][0]
                lines.append(f"ä»Šæ—¥é¢†æ¶¨: {top['name']}(+{top['change_pct']:.1f}%)")

            # èµ„é‡‘æµå‘
            if data['capital_flow']:
                top_flow = data['capital_flow'][0]
                lines.append(f"èµ„é‡‘æµå‘: {top_flow['sector']}å‡€æµå…¥{top_flow['net_inflow']:.1f}äº¿")

            return ", ".join(lines) if lines else "æ•°æ®è·å–ä¸­"

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆåˆ†æå¤±è´¥: {e}")
            return "åˆ†æå¤±è´¥"

    def analyze_market_sentiment(self, date: str = None) -> Dict:
        """
        åˆ†æå¸‚åœºæƒ…ç»ª

        Args:
            date: æ—¥æœŸ (YYYY-MM-DD), é»˜è®¤ä»Šå¤©

        Returns:
            å¸‚åœºæƒ…ç»ªåˆ†æç»“æœ
        """
        result = {
            'limit_up': 0,          # æ¶¨åœæ•°
            'limit_down': 0,        # è·Œåœæ•°
            'advance': 0,           # ä¸Šæ¶¨å®¶æ•°
            'decline': 0,           # ä¸‹è·Œå®¶æ•°
            'advance_decline_ratio': 0,  # æ¶¨è·Œæ¯”
            'northbound_flow': 0,   # åŒ—å‘èµ„é‡‘å‡€æµå…¥(äº¿)
            'northbound_trend': '', # åŒ—å‘èµ„é‡‘è¶‹åŠ¿
            'sentiment_score': 0,   # æƒ…ç»ªè¯„åˆ†(0-100)
            'sentiment_level': '',  # æƒ…ç»ªç­‰çº§
            'analysis': ''          # ç»¼åˆåˆ†æ
        }

        try:
            # 1. è·å–æ¶¨è·Œåœæ•°æ®ï¼ˆä¼˜å…ˆç”¨å®æ—¶è¡Œæƒ…ï¼Œå¤±è´¥åˆ™é™çº§ï¼‰
            self.logger.info("è·å–æ¶¨è·Œåœæ•°æ®...")
            data_fetched = False

            try:
                df_realtime = self._fetch_realtime_quotes_with_retry()

                if df_realtime is not None and len(df_realtime) > 0:
                    # ç»Ÿè®¡æ¶¨åœï¼ˆè€ƒè™‘æ™®é€šè‚¡å’ŒSTè‚¡çš„ä¸åŒæ¶¨å¹…é™åˆ¶ï¼‰
                    # æ™®é€šè‚¡ï¼šæ¶¨è·Œå¹… >= 9.9%
                    # STè‚¡ã€*STè‚¡ï¼šæ¶¨è·Œå¹… >= 4.9%

                    is_st = df_realtime['åç§°'].str.contains('ST|st', na=False, regex=True)

                    # æ™®é€šæ¶¨åœ
                    normal_limit_up = len(df_realtime[(~is_st) & (df_realtime['æ¶¨è·Œå¹…'] >= 9.9)])
                    # STæ¶¨åœ
                    st_limit_up = len(df_realtime[is_st & (df_realtime['æ¶¨è·Œå¹…'] >= 4.9)])
                    result['limit_up'] = normal_limit_up + st_limit_up

                    # æ™®é€šè·Œåœ
                    normal_limit_down = len(df_realtime[(~is_st) & (df_realtime['æ¶¨è·Œå¹…'] <= -9.9)])
                    # STè·Œåœ
                    st_limit_down = len(df_realtime[is_st & (df_realtime['æ¶¨è·Œå¹…'] <= -4.9)])
                    result['limit_down'] = normal_limit_down + st_limit_down

                    self.logger.info(f"âœ… æ¶¨åœ: {result['limit_up']}åª (æ™®é€š{normal_limit_up}+ST{st_limit_up})")
                    self.logger.info(f"âœ… è·Œåœ: {result['limit_down']}åª (æ™®é€š{normal_limit_down}+ST{st_limit_down})")

                    # åŒæ—¶ç»Ÿè®¡æ¶¨è·Œå®¶æ•°ï¼ˆå¤ç”¨æ•°æ®ï¼‰
                    result['advance'] = len(df_realtime[df_realtime['æ¶¨è·Œå¹…'] > 0])
                    result['decline'] = len(df_realtime[df_realtime['æ¶¨è·Œå¹…'] < 0])

                    if result['decline'] > 0:
                        result['advance_decline_ratio'] = result['advance'] / result['decline']

                    self.logger.info(f"âœ… ä¸Šæ¶¨: {result['advance']}åª, ä¸‹è·Œ: {result['decline']}åª")
                    data_fetched = True

            except Exception as e:
                self.logger.warning(f"å®æ—¶è¡Œæƒ…è·å–å¤±è´¥: {e}")

            # é™çº§ç­–ç•¥ï¼šå¦‚æœå®æ—¶è¡Œæƒ…å¤±è´¥ï¼Œä½¿ç”¨æ¶¨åœæ± æ¥å£ï¼ˆè‡³å°‘æœ‰éƒ¨åˆ†æ•°æ®ï¼‰
            if not data_fetched:
                self.logger.info("âš ï¸ é™çº§åˆ°æ¶¨åœæ± æ¥å£...")
                try:
                    df_limit_up = ak.stock_zt_pool_em(date=datetime.now().strftime('%Y%m%d'))
                    if df_limit_up is not None:
                        result['limit_up'] = len(df_limit_up)
                        self.logger.info(f"âœ… æ¶¨åœ: {result['limit_up']}åª (æ¥æº:æ¶¨åœæ± )")
                        # æ³¨ï¼šè·Œåœæ•°æ®ä»ä¸º0ï¼Œå› ä¸ºæ²¡æœ‰å¯¹åº”æ¥å£
                except Exception as e2:
                    self.logger.warning(f"æ¶¨åœæ± æ¥å£ä¹Ÿå¤±è´¥: {e2}")

            # 3. è·å–åŒ—å‘èµ„é‡‘
            try:
                df_northbound = ak.stock_hsgt_fund_flow_summary_em()
                if df_northbound is not None and len(df_northbound) > 0:
                    # è·å–æœ€æ–°ä¸€å¤©çš„æ•°æ®
                    latest = df_northbound.iloc[0]
                    result['northbound_flow'] = float(latest['åŒ—å‘èµ„é‡‘']) / 1e8  # è½¬æ¢ä¸ºäº¿

                    # åˆ¤æ–­è¶‹åŠ¿(éœ€è¦å†å²æ•°æ®)
                    if len(df_northbound) >= 3:
                        recent_flows = [float(df_northbound.iloc[i]['åŒ—å‘èµ„é‡‘']) for i in range(3)]
                        if all(f > 0 for f in recent_flows):
                            result['northbound_trend'] = 'è¿ç»­æµå…¥'
                        elif all(f < 0 for f in recent_flows):
                            result['northbound_trend'] = 'è¿ç»­æµå‡º'
                        else:
                            result['northbound_trend'] = 'éœ‡è¡'

                    self.logger.info(f"âœ… åŒ—å‘èµ„é‡‘: {result['northbound_flow']:.1f}äº¿")

            except Exception as e:
                self.logger.warning(f"è·å–åŒ—å‘èµ„é‡‘å¤±è´¥: {e}")

            # 4. è®¡ç®—æƒ…ç»ªè¯„åˆ†
            result['sentiment_score'] = self._calculate_sentiment_score(result)
            result['sentiment_level'] = self._get_sentiment_level(result['sentiment_score'])

            # 5. ç”Ÿæˆç»¼åˆåˆ†æ
            result['analysis'] = self._generate_sentiment_analysis(result)

        except Exception as e:
            self.logger.error(f"å¸‚åœºæƒ…ç»ªåˆ†æå¤±è´¥: {e}", exc_info=True)

        return result

    def _calculate_sentiment_score(self, data: Dict) -> int:
        """
        è®¡ç®—å¸‚åœºæƒ…ç»ªè¯„åˆ† (0-100)

        è¯„åˆ†è§„åˆ™:
        - æ¶¨è·Œæ¯”: 40åˆ†
        - æ¶¨è·Œåœæ¯”: 20åˆ†
        - åŒ—å‘èµ„é‡‘: 40åˆ†
        """
        score = 50  # åŸºç¡€åˆ†

        try:
            # 1. æ¶¨è·Œæ¯”è´¡çŒ® (æœ€å¤š40åˆ†)
            if data['advance_decline_ratio'] > 0:
                ratio_score = min(data['advance_decline_ratio'] * 20, 40)
                score += ratio_score - 20  # è°ƒæ•´ä¸ºç›¸å¯¹åˆ†æ•°

            # 2. æ¶¨è·Œåœæ¯”è´¡çŒ® (æœ€å¤š20åˆ†)
            if data['limit_up'] > 0 or data['limit_down'] > 0:
                limit_ratio = data['limit_up'] / max(data['limit_down'], 1)
                limit_score = min(limit_ratio * 10, 20)
                score += limit_score - 10

            # 3. åŒ—å‘èµ„é‡‘è´¡çŒ® (æœ€å¤š40åˆ†)
            if data['northbound_flow'] != 0:
                # æµå…¥ä¸ºæ­£,æµå‡ºä¸ºè´Ÿ
                nb_score = max(-20, min(20, data['northbound_flow'] / 10))
                score += nb_score

            # é™åˆ¶åœ¨0-100èŒƒå›´
            score = max(0, min(100, score))

        except Exception as e:
            self.logger.warning(f"æƒ…ç»ªè¯„åˆ†è®¡ç®—å¤±è´¥: {e}")
            score = 50

        return int(score)

    def _get_sentiment_level(self, score: int) -> str:
        """è·å–æƒ…ç»ªç­‰çº§"""
        if score >= 80:
            return "æåº¦ä¹è§‚(è°¨é˜²è¿½é«˜)"
        elif score >= 65:
            return "åçƒ­(æ³¨æ„è¿½é«˜é£é™©)"
        elif score >= 50:
            return "ä¸­æ€§åæš–"
        elif score >= 35:
            return "åå†·"
        else:
            return "æåº¦æ‚²è§‚(å…³æ³¨æŠ„åº•æœºä¼š)"

    def _generate_sentiment_analysis(self, data: Dict) -> str:
        """ç”Ÿæˆå¸‚åœºæƒ…ç»ªç»¼åˆåˆ†æ"""
        try:
            lines = []

            # æ¶¨è·Œæ¯”åˆ†æ
            if data['advance_decline_ratio'] > 1.5:
                lines.append(f"æ¶¨è·Œæ¯”{data['advance_decline_ratio']:.1f}:1,å¸‚åœºåå¤š")
            elif data['advance_decline_ratio'] < 0.8:
                lines.append(f"æ¶¨è·Œæ¯”{data['advance_decline_ratio']:.1f}:1,å¸‚åœºåç©º")

            # åŒ—å‘èµ„é‡‘åˆ†æ
            if abs(data['northbound_flow']) > 50:
                direction = "æµå…¥" if data['northbound_flow'] > 0 else "æµå‡º"
                lines.append(f"åŒ—å‘èµ„é‡‘å¤§å¹…{direction}{abs(data['northbound_flow']):.0f}äº¿")

            return ", ".join(lines) if lines else "å¸‚åœºæƒ…ç»ªä¸­æ€§"

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆåˆ†æå¤±è´¥: {e}")
            return "åˆ†æå¤±è´¥"

    def analyze_volume_price(self, date: str = None) -> Dict:
        """
        é‡ä»·å…³ç³»æ·±åº¦åˆ†æ

        Args:
            date: æ—¥æœŸ (YYYY-MM-DD), é»˜è®¤ä»Šå¤©

        Returns:
            é‡ä»·å…³ç³»åˆ†æç»“æœ
        """
        result = {
            'hs300_volume': 0,          # æ²ªæ·±300æˆäº¤é¢(ä¸‡äº¿)
            'hs300_volume_ratio': 0,    # æ²ªæ·±300é‡æ¯”
            'cyb_volume': 0,            # åˆ›ä¸šæ¿æˆäº¤é¢(äº¿)
            'cyb_volume_ratio': 0,      # åˆ›ä¸šæ¿é‡æ¯”
            'total_turnover': 0,        # ä¸¤å¸‚æ€»æˆäº¤é¢(ä¸‡äº¿)
            'main_net_inflow': 0,       # ä¸»åŠ›èµ„é‡‘å‡€æµå…¥(äº¿)
            'large_order_ratio': 0,     # å¤§å•å æ¯”
            'conclusion': '',           # ç»“è®º
            'analysis': ''              # ç»¼åˆåˆ†æ
        }

        try:
            # 1. è·å–ä¸¤å¸‚æˆäº¤é¢
            self.logger.info("è·å–æˆäº¤é¢æ•°æ®...")
            try:
                # ä½¿ç”¨efinanceè·å–å®æ—¶æ•°æ®
                df_market = ef.stock.get_realtime_quotes()
                if df_market is not None and len(df_market) > 0:
                    # è®¡ç®—æ€»æˆäº¤é¢(å•ä½:ä¸‡äº¿)
                    total_amount = df_market['æˆäº¤é¢'].sum() / 1e12
                    result['total_turnover'] = total_amount
                    self.logger.info(f"âœ… ä¸¤å¸‚æˆäº¤é¢: {total_amount:.2f}ä¸‡äº¿")

            except Exception as e:
                self.logger.warning(f"è·å–æˆäº¤é¢å¤±è´¥: {e}")

            # 2. è·å–ä¸»åŠ›èµ„é‡‘æµå‘
            try:
                df_flow = ak.stock_individual_fund_flow_rank(indicator="ä»Šæ—¥")
                if df_flow is not None and len(df_flow) > 0:
                    # è®¡ç®—ä¸»åŠ›èµ„é‡‘å‡€æµå…¥æ€»é¢
                    result['main_net_inflow'] = df_flow['ä¸»åŠ›å‡€æµå…¥-å‡€é¢'].sum() / 1e8

                    # è®¡ç®—å¤§å•å æ¯”
                    total_inflow = df_flow['ä¸»åŠ›å‡€æµå…¥-å‡€é¢'].sum()
                    large_order = df_flow['è¶…å¤§å•å‡€æµå…¥-å‡€é¢'].sum()
                    if total_inflow != 0:
                        result['large_order_ratio'] = (large_order / total_inflow) * 100

                    self.logger.info(f"âœ… ä¸»åŠ›èµ„é‡‘å‡€æµå…¥: {result['main_net_inflow']:.1f}äº¿")

            except Exception as e:
                self.logger.warning(f"è·å–ä¸»åŠ›èµ„é‡‘å¤±è´¥: {e}")

            # 3. ç”Ÿæˆç»“è®ºå’Œåˆ†æ
            result['conclusion'] = self._generate_volume_conclusion(result)
            result['analysis'] = self._generate_volume_analysis(result)

        except Exception as e:
            self.logger.error(f"é‡ä»·å…³ç³»åˆ†æå¤±è´¥: {e}", exc_info=True)

        return result

    def _generate_volume_conclusion(self, data: Dict) -> str:
        """ç”Ÿæˆé‡ä»·å…³ç³»ç»“è®º"""
        try:
            if data['main_net_inflow'] > 100:
                return "æ”¾é‡ä¸Šæ¶¨,èµ„é‡‘é¢å¼ºåŠ›æ”¯æŒ"
            elif data['main_net_inflow'] > 0:
                return "æ¸©å’Œæ”¾é‡,èµ„é‡‘é¢æ”¯æŒ"
            elif data['main_net_inflow'] > -50:
                return "ç¼©é‡éœ‡è¡,è§‚æœ›æƒ…ç»ªæµ“åš"
            else:
                return "æ”¾é‡ä¸‹è·Œ,è°¨æ…è§‚æœ›"
        except:
            return "æ•°æ®ä¸è¶³"

    def _generate_volume_analysis(self, data: Dict) -> str:
        """ç”Ÿæˆé‡ä»·å…³ç³»ç»¼åˆåˆ†æ"""
        try:
            lines = []

            # æˆäº¤é¢åˆ†æ
            if data['total_turnover'] > 2.0:
                lines.append(f"ä¸¤å¸‚æˆäº¤{data['total_turnover']:.2f}ä¸‡äº¿(æ”¾é‡)")
            elif data['total_turnover'] > 1.5:
                lines.append(f"ä¸¤å¸‚æˆäº¤{data['total_turnover']:.2f}ä¸‡äº¿(æ¸©å’Œ)")
            else:
                lines.append(f"ä¸¤å¸‚æˆäº¤{data['total_turnover']:.2f}ä¸‡äº¿(ç¼©é‡)")

            # èµ„é‡‘æµå‘
            if abs(data['main_net_inflow']) > 50:
                direction = "å‡€æµå…¥" if data['main_net_inflow'] > 0 else "å‡€æµå‡º"
                lines.append(f"ä¸»åŠ›èµ„é‡‘{direction}{abs(data['main_net_inflow']):.0f}äº¿")

            return ", ".join(lines) if lines else "æ•°æ®è·å–ä¸­"

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆåˆ†æå¤±è´¥: {e}")
            return "åˆ†æå¤±è´¥"

    def generate_depth_report(self, date: str = None, market_data: Dict = None) -> str:
        """
        ç”Ÿæˆæ·±åº¦ç›˜é¢åˆ†ææŠ¥å‘Š

        Args:
            date: æ—¥æœŸ
            market_data: å¸‚åœºæ•°æ®(åŒ…å«æˆäº¤é¢ç­‰ä¿¡æ¯)

        Returns:
            Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        """
        lines = []

        lines.append("## ğŸ“Š æ·±åº¦ç›˜é¢åˆ†æ")
        lines.append("")

        # 1. æ¿å—è½®åŠ¨
        lines.append("### ğŸ”¥ æ¿å—è½®åŠ¨è¿½è¸ª")
        lines.append("")

        sector_data = self.analyze_sector_rotation(date)

        # æ•°æ®æ¥æºæç¤º
        if sector_data.get('data_source') == 'cache':
            cache_hint = " *(ä½¿ç”¨ç¼“å­˜æ•°æ®)*"
            lines.append("> âš ï¸ **æç¤º**: å®æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œå½“å‰ä½¿ç”¨ç¼“å­˜æ•°æ®")
            lines.append("")
        else:
            cache_hint = ""

        # é¢†æ¶¨æ¿å—
        if sector_data['top_gainers']:
            lines.append(f"**ä»Šæ—¥é¢†æ¶¨æ¿å—**{cache_hint}:")
            lines.append("")
            for sector in sector_data['top_gainers'][:5]:
                emoji = "ğŸ”¥" if sector['change_pct'] > 3 else "ğŸ“ˆ"
                lines.append(f"- {emoji} **{sector['name']}**: +{sector['change_pct']:.2f}%")
            lines.append("")

        # é¢†è·Œæ¿å—
        if sector_data['top_losers']:
            lines.append("**ä»Šæ—¥é¢†è·Œæ¿å—**:")
            lines.append("")
            for sector in sector_data['top_losers'][:5]:
                emoji = "ğŸ’¥" if sector['change_pct'] < -3 else "ğŸ“‰"
                lines.append(f"- {emoji} **{sector['name']}**: {sector['change_pct']:.2f}%")
            lines.append("")

        # èµ„é‡‘æµå‘
        if sector_data['capital_flow']:
            lines.append("**ä¸»åŠ›èµ„é‡‘æµå‘**:")
            lines.append("")
            for flow in sector_data['capital_flow'][:3]:
                emoji = "ğŸ’°" if flow['net_inflow'] > 0 else "ğŸ’¸"
                lines.append(f"- {emoji} {flow['sector']}: {flow['net_inflow']:+.1f}äº¿ ({flow['net_inflow_pct']:+.1f}%)")
            lines.append("")

        # è½®åŠ¨ä¿¡å·
        if sector_data['rotation_signal']:
            lines.append(f"**è½®åŠ¨ä¿¡å·**: {sector_data['rotation_signal']}")
            lines.append("")

        lines.append("---")
        lines.append("")

        # 2. å¸‚åœºæƒ…ç»ª
        lines.append("### ğŸŒ¡ï¸ å¸‚åœºæƒ…ç»ªæ¸©åº¦è®¡")
        lines.append("")

        sentiment_data = self.analyze_market_sentiment(date)

        lines.append("| æŒ‡æ ‡ | æ•°æ® | è¯´æ˜ |")
        lines.append("|------|------|------|")

        # æ¶¨è·Œåœ
        lines.append(f"| æ¶¨åœ/è·Œåœ | {sentiment_data['limit_up']}åª / {sentiment_data['limit_down']}åª | "
                    f"{'ğŸ”¥ æƒ…ç»ªç«çƒ­' if sentiment_data['limit_up'] > 50 else 'â¡ï¸ æ­£å¸¸' if sentiment_data['limit_up'] > 20 else 'â„ï¸ æƒ…ç»ªå†·æ·¡'} |")

        # æ¶¨è·Œå®¶æ•°
        if sentiment_data['advance'] > 0 or sentiment_data['decline'] > 0:
            lines.append(f"| ä¸Šæ¶¨/ä¸‹è·Œ | {sentiment_data['advance']}åª / {sentiment_data['decline']}åª | "
                        f"æ¶¨è·Œæ¯” {sentiment_data['advance_decline_ratio']:.1f}:1 |")

        # åŒ—å‘èµ„é‡‘
        if sentiment_data['northbound_flow'] != 0:
            direction = "å‡€æµå…¥" if sentiment_data['northbound_flow'] > 0 else "å‡€æµå‡º"
            emoji = "ğŸ’°" if sentiment_data['northbound_flow'] > 0 else "ğŸ’¸"
            trend = f"({sentiment_data['northbound_trend']})" if sentiment_data['northbound_trend'] else ""
            lines.append(f"| åŒ—å‘èµ„é‡‘ | {emoji} {direction} Â¥{abs(sentiment_data['northbound_flow']):.1f}äº¿ | {trend} |")

        # æƒ…ç»ªè¯„åˆ†
        lines.append(f"| **æƒ…ç»ªè¯„åˆ†** | **{sentiment_data['sentiment_score']}åˆ†** | {sentiment_data['sentiment_level']} |")

        lines.append("")

        if sentiment_data['analysis']:
            lines.append(f"**ç»¼åˆåˆ¤æ–­**: {sentiment_data['analysis']}")
            lines.append("")

        lines.append("---")
        lines.append("")

        # 3. é‡ä»·å…³ç³»
        lines.append("### ğŸ“ˆ é‡ä»·å…³ç³»æ·±åº¦")
        lines.append("")

        volume_data = self.analyze_volume_price(date)

        # å¦‚æœmarket_dataæœ‰æˆäº¤é¢æ•°æ®,ä¼˜å…ˆä½¿ç”¨
        if market_data:
            if 'market_volume' in market_data and 'total_volume_trillion' in market_data['market_volume']:
                turnover = market_data['market_volume']['total_volume_trillion']
                if turnover > 0:
                    self.logger.info(f"ä½¿ç”¨market_dataä¸­çš„æˆäº¤é¢: {turnover:.2f}ä¸‡äº¿")
                    volume_data['total_turnover'] = turnover

            # å¦‚æœæœ‰ä¸»åŠ›èµ„é‡‘æ•°æ®,ä¹Ÿä½¿ç”¨
            if 'fund_flow' in market_data and market_data['fund_flow'] and 'main_net_inflow' in market_data['fund_flow']:
                main_inflow_yuan = market_data['fund_flow']['main_net_inflow']
                main_inflow_yi = main_inflow_yuan / 100000000  # è½¬æ¢ä¸ºäº¿
                self.logger.info(f"ä½¿ç”¨market_dataä¸­çš„ä¸»åŠ›èµ„é‡‘: {main_inflow_yi:.2f}äº¿")
                volume_data['main_net_inflow'] = main_inflow_yi

            # é‡æ–°ç”Ÿæˆåˆ†æå’Œç»“è®º
            volume_data['conclusion'] = self._generate_volume_conclusion(volume_data)
            volume_data['analysis'] = self._generate_volume_analysis(volume_data)

        lines.append("| æŒ‡æ ‡ | æ•°æ® | åˆ†æ |")
        lines.append("|------|------|------|")

        # ä¸¤å¸‚æˆäº¤é¢
        if volume_data['total_turnover'] > 0:
            volume_level = "ğŸ”¥ æ”¾é‡" if volume_data['total_turnover'] > 2.0 else "â¡ï¸ æ¸©å’Œ" if volume_data['total_turnover'] > 1.5 else "â„ï¸ ç¼©é‡"
            lines.append(f"| ä¸¤å¸‚æˆäº¤é¢ | {volume_data['total_turnover']:.2f}ä¸‡äº¿ | {volume_level} |")

        # ä¸»åŠ›èµ„é‡‘
        if volume_data['main_net_inflow'] != 0:
            direction = "å‡€æµå…¥" if volume_data['main_net_inflow'] > 0 else "å‡€æµå‡º"
            emoji = "ğŸ’°" if volume_data['main_net_inflow'] > 0 else "ğŸ’¸"
            lines.append(f"| ä¸»åŠ›èµ„é‡‘ | {emoji} {direction} Â¥{abs(volume_data['main_net_inflow']):.1f}äº¿ | "
                        f"{'å¤§å•ä¸»å¯¼' if volume_data['large_order_ratio'] > 60 else 'æ•£æˆ·ä¸»å¯¼'} |")

        # ç»“è®º (åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰æ˜¾ç¤º)
        if volume_data['total_turnover'] > 0 or volume_data['main_net_inflow'] != 0:
            lines.append(f"| **ç»“è®º** | **{volume_data['conclusion']}** | - |")

        lines.append("")

        # é‡ä»·åˆ†æ (åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰æ˜¾ç¤º)
        if volume_data['analysis'] and (volume_data['total_turnover'] > 0 or volume_data['main_net_inflow'] != 0):
            lines.append(f"**é‡ä»·åˆ†æ**: {volume_data['analysis']}")
            lines.append("")
        elif volume_data['total_turnover'] == 0 and volume_data['main_net_inflow'] == 0:
            lines.append(f"**é‡ä»·åˆ†æ**: æ•°æ®è·å–å¤±è´¥,è¯·ç¨åé‡è¯•")
            lines.append("")

        lines.append("---")
        lines.append("")

        return '\n'.join(lines)


if __name__ == '__main__':
    # æµ‹è¯•
    logging.basicConfig(level=logging.INFO)

    analyzer = MarketDepthAnalyzer()
    report = analyzer.generate_depth_report()

    print(report)
